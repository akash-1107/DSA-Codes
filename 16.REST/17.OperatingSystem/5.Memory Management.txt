() = file
   = process
1.How a program is compiled and run
ans = (Source Code) -> compiler(convert src code to machine dependent object code) 
->(object code generated) -> linker(link object code of other functions or library and generate ) ->
 (executable code) -> loader(put execuatble code in memory) -> Run in Memory

 2.Memory Management in OS - 
   0.Judged - 1.Acess time(kam chaiye)
              2.capacity(jada hona chahiye)
              3.cost(kam)
   1.Memory Hierchahy(based on access time(jo kam hona chahiye)) - 1.Cache
                                                                   2.Main Memory
                                                                   3.Secondary Memory (jitta kam access time utna pass CPU ka)
   2.Address Binding -
   (krna jarurui hai taki hm context switching load unload into memory)
   (Binary Executable file hota hai usme binary ko memory address me convert krta hai)
   1.Compile Time Binding (manually put kr rhe in memory address a/q to given value only) 
   2.Load Time Binding (relative address ke hisab se jaha load whi ke hisab se adjust ho jayega pura code)
   3.Run Time Binding(in Modern System) - 1.Generate a Logical Address -> phir hardware se ek physical address generate
                                          2.Done using hardware (cant be done by OS)(MMU -> ek limit register and ek relocation register hai)
                                          3.kyuki context switch me hm load unload krenge na ki os switch krenge (isiliye hardware se kro)
   
   prblms - 1.External Fragmentation - fragments milake jagah h par allocate nhi kr skte uss jagah 
            2.Internal Fragmentation - jarurat se jada allocate to a process
            
   3.Memory allocation in multitasking(basically jada se jada process ko cpu me lana h ) - 
            1.Static Allocation - (at compile time )
             1.Equal size (sb 5-5 ka hogya) (both prblms h isme)
             2.Unequal size(koi 5,6,2) (both prblms h isme)

            2.Dynamic Allocation - (at run time decided)
            1.Contigous Allocation - prblm of External Allocation ho jayega not internal
            2.Non-contigous Allocation - 1.Paging 
                                         2.Segmentation
                                         3.Paging With Segmentation

   4.Dynamic partitioning me kon sa jagah khali h pta rakhne ke liye i l=use doubly linked list to link process allocated wala space and holes using doubly linked list

5.
MMU ko chahiye ki contigous allocation ho par hm dynamic me page in virtual and frame in main Memory non contigous
h frame alag alag jagah se aa rha hai isiliye MMU need (page table = (map b/w page and frame)) to convert logical to physical.  page size = frame size

1.Paging:
Paging is a memory management scheme used in computer operating systems and hardware.
Memory is divided into fixed-size blocks called "frames" in physical(main) memory and "pages" in logical memory.
The main idea is to break up the physical memory and logical memory into equal-sized blocks to simplify memory management.

2.Page Table:
A page table is a data structure used by the memory management unit (MMU) to translate logical addresses (virtual addresses) into physical addresses.
It contains entries that map logical page numbers to corresponding physical frame numbers.
When a program generates a logical address, the MMU consults the page table to find the associated physical address.

3.Logical Address:
A logical address, also known as a virtual address, is an address generated by the CPU during program execution.
It's used by the program to access memory locations without needing to know the actual physical memory layout.

4.Page Fault:
A page fault occurs when a program tries to access a page of memory that is not currently in physical memory (RAM).
This might happen when a program accesses a page that was swapped out to secondary storage or hasn't been loaded into memory yet.
When a page fault occurs, the operating system needs to bring the required page into physical memory (usually by swapping out another page) before allowing the program to continue execution.

6.Virtual Memory - (Virtual memory helps large applications run on systems with limited RAM)

Memory Management: Virtual memory is a memory management technique used by operating systems to provide an illusion of larger memory space than physically available RAM.

Address Space: Each program in execution has its own virtual address space, which is divided into smaller units called "pages." These pages are usually 4KB in size.

Physical RAM: Physical memory (RAM) is limited, but virtual memory allows the operating system to use a portion of the hard drive as if it were additional RAM.

Page Table: The operating system maintains a data structure called a "page table" that maps virtual addresses to physical addresses in RAM or on disk.

Page Faults: When a program accesses a virtual address that is not currently in physical RAM, a page fault occurs. The operating system then loads the required page from disk into RAM.
(very costly --> increases access time hugely)

Swapping: If physical RAM becomes full, the operating system may "swap out" some pages from RAM to disk to make room for new pages. This process is called paging.

Demand Paging: Not all pages are loaded into RAM at once; only the pages that are actively used are loaded. This optimizes memory usage.

7.
TLB (Translation Lookaside Buffer):  (recently wala store rakhte so quick access)
TLB is a cache that stores recently used page table entries, reducing the need to access the main page table.
It speeds up the virtual-to-physical address translation process by providing a quick lookup mechanism.
TLB helps improve memory access times and overall system performance.

Demand Paging: ()
Demand paging loads only the required pages of a program into memory, rather than loading the entire program.
It optimizes memory usage by fetching pages as they are needed, minimizing initial loading time.
Page faults trigger the loading of missing pages from disk into memory.

Thrashing:(more process ram me lane se DOMP increase oar ek point ke baad page faults increases very much crash)
Thrashing occurs when the system spends more time swapping pages in and out of memory than executing actual tasks.
It results in extremely poor performance due to high disk I/O and low CPU utilization.
Thrashing often happens when the degree of multiprogramming exceeds the capacity of physical memory, causing frequent page faults.

Page Replacement Algorithms:
These algorithms determine which page to evict from memory when a new page needs to be loaded and memory is full.
Common algorithms include:
LRU (Least Recently Used): Replaces the page that hasn't been used for the longest time.(best option)
FIFO (First-In-First-Out): Replaces the oldest page in memory.
Optimal: Replaces the page that won't be used for the longest time in the future (theoretical but not practical).
LFU (Least Frequently Used): Replaces the page with the lowest access frequency.
Random: Randomly selects a page to replace.
Choosing the right algorithm depends on factors like memory access patterns and system characteristics.

8.Belady's Anomaly in the FIFO algorithm is when increasing available memory frames can paradoxically lead to more page faults, as older pages that were previously evicted end up being replaced more frequently due to the peculiar order of page references in the sequence.

ex - Of course! Imagine your computer's memory is like a limited bookshelf where it can store only a certain number of books at a time. Each book represents a "page" of data that a program needs to use.

Now, let's say you're using the FIFO method, where the oldest book on the shelf is taken out to make space for a new one whenever needed. Here's the twist: sometimes, if you add more shelves (memory frames), you might find that the books you put away earlier keep coming back more often, even though you expected the extra shelves to make things smoother.

This puzzling situation is Belady's Anomaly. Despite adding more memory space, the way you're swapping out old pages can sometimes lead to more work (page faults) instead of less, making your computer slower. It's like trying to organize your shelves in a way that ends up making things messier. This anomaly highlights how memory management can have counterintuitive effects, and it's a reminder that simple solutions don't always yield the expected results.

9.Segmentation-
Certainly! Segmentation is a memory management technique used by some operating systems to organize and manage memory in a more flexible way. It divides a program's address space into segments, each representing a different part of the program's functionality. Here's an explanation:

Address Space Division:
In segmentation, a program's address space is divided into different segments, each serving a specific purpose or containing a particular type of data, such as code, data, stack, and more.

Segments:
Segments are logical units that represent different parts of a program. For example, you might have a code segment for program instructions, a data segment for variables, and a stack segment for managing function calls and local variables.

Variable Sizes:
Unlike paging, where all pages have the same size, segments can have variable sizes. This allows more precise allocation of memory based on the needs of each segment.

Segment Table:
The operating system maintains a segment table that maps each segment to its corresponding physical memory location. Each entry in the table holds the base address and length of the segment.

Address Translation:
When a program references a memory location using a segment and an offset, the segment table is consulted to translate the segment-based address into a physical memory address.

External Fragmentation:
One drawback of segmentation is external fragmentation. As programs load and unload segments of different sizes, free memory blocks become scattered, which can lead to inefficient memory usage.

10.benefits of segmentation - 
Certainly, here are the benefits of segmentation over paging in concise points:

Variable-sized Allocation: Segmentation allows different parts of a program to use memory of sizes tailored to their needs.
Logical Organization: Segmentation reflects the logical structure of a program, making it easier to manage memory for various components.
Sharing and Protection: Segmentation offers finer-grained control over memory sharing and protection by applying permissions to individual segments.
Flexibility for Growing Programs: Segmentation accommodates growing programs without the need for continuous, fixed-size blocks.
Suitable for Diverse Applications: Segmentation is beneficial for applications with varying memory requirements, like compilers and operating systems.


11.Certainly, here's a comparison of segmentation over paging in concise points -

Memory Allocation:
Segmentation: Variable-sized allocation for different parts of a program.
Paging: Fixed-sized allocation using pages throughout the program's address space.

Memory Organization:
Segmentation: Reflects the logical structure of a program with separate segments for code, data, stack, etc.
Paging: Divides the memory space into fixed-size pages, without considering program structure.

Fragmentation:
Segmentation: Can suffer from external fragmentation due to varying segment sizes.
Paging: Minimizes fragmentation, as all pages are of the same size.

Memory Protection:
Segmentation: Offers finer control over memory protection and sharing at the segment level.
Paging: Provides protection and sharing at the page level.

Flexibility:
Segmentation: Allows for dynamic allocation of segments as the program grows.
Paging: Requires continuous, fixed-size blocks, which might lead to inefficient use of memory.

Program Types:
Segmentation: Suited for applications with diverse memory requirements.
Paging: Generally suitable for applications with uniform memory access patterns.

12.segmentation with paging-
(har segment me paging lagao and basically har segment ka apna page table hai and sb ek common segment table se connected hai)
logical address me -> segment ->page->usme offset

Sure, let's delve into how segmentation with paging works using a combination of page tables and a generalized segment table:

Segmentation Overview:
Segmentation divides a program's address space into segments, each representing a distinct part of the program (e.g. code, data, stack).

Segment Table:
A generalized segment table holds entries for each segment.
Each entry includes the base address and length of the segment, along with access permissions.

Paging Within Segments:
Within each segment, memory is further divided into fixed-size blocks called pages (e.g., 4KB each).

Page Table for Each Segment:
For every segment, there's a separate page table.
Page tables map virtual page numbers within a segment to physical page numbers in memory.
